# .github/workflows/daily-scrape.yml
name: Daily Scrape & Dashboard

permissions:
  contents: write
  pages: write

on:
  push:
    branches:
      - main
    paths-ignore:
      - today_listings.csv
      - historical_listings.csv
      - docs/**
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  build_and_deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install requests beautifulsoup4 pandas matplotlib tqdm

      - name: Scrape listings
        run: python scraper.py

      - name: Merge & dedupe
        shell: bash
        run: |
          python - << 'EOF'
          import pandas as pd, os

          today = pd.read_csv("today_listings.csv")
          if os.path.exists("historical_listings.csv"):
              hist = pd.read_csv("historical_listings.csv")
              combined = pd.concat([hist, today], ignore_index=True)
          else:
              combined = today

          combined.drop_duplicates(subset=["scrape_date","listing_url"], inplace=True)
          combined.to_csv("historical_listings.csv", index=False)
          EOF

      - name: Generate Dashboard
        run: python generate_dashboard.py
        env:
          TZ: Europe/Tirane

      - name: Deploy to gh-pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: docs
          publish_branch: gh-pages
